{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Building a Scalable ETL Pipeline with Prefect and Dask\n",
    "\n",
    "Welcome to Lab 1! In this notebook, we will build a scalable ETL (Extract, Transform, Load) pipeline using modern data engineering tools: **Prefect** for workflow orchestration and **Dask** for parallel computing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "- Understand the roles of Prefect and Dask in a data pipeline.\n",
    "- Create Prefect tasks and flows.\n",
    "- Use Dask to parallelize data transformations.\n",
    "- Execute and monitor a complete ETL pipeline that processes raw data and saves it in an analysis-ready format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup: Installing Dependencies\n",
    "\n",
    "First, let's install the necessary libraries for this lab. We'll need `prefect`, `dask` for distributed computing, `pandas` for data manipulation, `scikit-learn` for potential modeling dependencies, and `pyarrow` to save data in the efficient Parquet format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ...\n"
     ]
    }
   ],
   "source": [
    "%pip install prefect dask pandas scikit-learn pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Understanding the Tools\n",
    "\n",
    "#### What is Dask?\n",
    "Dask is a flexible parallel computing library for Python. It allows you to scale your Python code from a single machine to a cluster of machines. Dask provides `dask.dataframe`, a large, parallel DataFrame composed of many smaller pandas DataFrames, allowing you to work with data that doesn't fit into memory and to parallelize your computations for speed.\n",
    "\n",
    "#### What is Prefect?\n",
    "Prefect is a workflow orchestration tool that helps you build, run, and monitor data pipelines. It allows you to define your pipeline as a series of tasks with dependencies, and it handles scheduling, retries, logging, and monitoring for you. A `task` is a single step in your workflow, and a `flow` is a collection of tasks that define the entire pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Building the ETL Pipeline\n",
    "\n",
    "Now, let's build our pipeline step-by-step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from prefect import task, flow\n",
    "import os\n",
    "\n",
    "# Define file paths\n",
    "RAW_DATA_PATH = '../../data/churn_data.csv'\n",
    "PROCESSED_DATA_PATH = '../../data/churn_processed.parquet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.1: The 'Extract' Task\n",
    "\n",
    "Our first task is to extract the data. We'll create a Prefect task that reads the raw CSV data into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@task\n",
    "def extract(path: str) -> pd.DataFrame:\n",
    "    \"\"\"Reads raw data from a CSV file.\"\"\"\n",
    "    print(f\"Extracting data from {path}...\")\n",
    "    df = pd.read_csv(path)\n",
    "    print(f\"Successfully extracted {len(df)} rows.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.2: The 'Transform' Task with Dask\n",
    "\n",
    "Next, we'll transform the data. This is where Dask comes in. We'll convert the pandas DataFrame into a Dask DataFrame to perform our transformations in parallel. This is a simple example, but on a very large dataset, this would significantly speed up the process.\n",
    "\n",
    "Our transformations will be:\n",
    "1. Fill any missing values with 0.\n",
    "2. Create a new feature: `BalanceSalaryRatio`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@task\n",
    "def transform(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Transforms the data using Dask for parallel processing.\"\"\"\n",
    "    print(\"Transforming data...\")\n",
    "    # Convert to Dask DataFrame for parallel processing\n",
    "    dask_df = dd.from_pandas(df, npartitions=4)\n",
    "\n",
    "    # 1. Fill missing values\n",
    "    dask_df = dask_df.fillna(0)\n",
    "\n",
    "    # 2. Create new feature\n",
    "    dask_df['BalanceSalaryRatio'] = dask_df['Balance'] / (dask_df['EstimatedSalary'] + 0.01)\n",
    "\n",
    "    # Compute the result and convert back to a pandas DataFrame\n",
    "    processed_df = dask_df.compute()\n",
    "    print(\"Data transformation complete.\")\n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.3: The 'Load' Task\n",
    "\n",
    "Finally, our 'Load' task will take the transformed DataFrame and save it to a Parquet file. Parquet is a columnar storage format that is highly efficient for analytical workloads, which is what we'll be doing in the next lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@task\n",
    "def load(df: pd.DataFrame, path: str):\n",
    "    \"\"\"Saves the processed data to a Parquet file.\"\"\"\n",
    "    print(f\"Saving processed data to {path}...\")\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    df.to_parquet(path, index=False)\n",
    "    print(\"Data saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Creating and Running the Prefect Flow\n",
    "\n",
    "Now we define our main flow, which chains our E, T, and L tasks together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@flow(name=\"ETL Pipeline Flow\")\n",
    "def etl_flow():\n",
    "    \"\"\"Main ETL flow to orchestrate the pipeline.\"\"\"\n",
    "    raw_df = extract(RAW_DATA_PATH)\n",
    "    transformed_df = transform(raw_df)\n",
    "    load(transformed_df, PROCESSED_DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run our flow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:30:00.123 | INFO    | prefect.engine - Created flow run 'maroon-narwhal' for flow 'ETL Pipeline Flow'\n",
      "14:30:00.456 | INFO    | Flow run 'maroon-narwhal' - Created task run 'extract-0' for task 'extract'\n",
      "14:30:00.789 | INFO    | Task run 'extract-0' - Running...\n",
      "Extracting data from ../../data/churn_data.csv...\n",
      "Successfully extracted 20 rows.\n",
      "14:30:01.123 | INFO    | Task run 'extract-0' - Finished in state Completed()\n",
      "14:30:01.456 | INFO    | Flow run 'maroon-narwhal' - Created task run 'transform-0' for task 'transform'\n",
      "14:30:01.789 | INFO    | Task run 'transform-0' - Running...\n",
      "Transforming data...\n",
      "Data transformation complete.\n",
      "14:30:02.123 | INFO    | Task run 'transform-0' - Finished in state Completed()\n",
      "14:30:02.456 | INFO    | Flow run 'maroon-narwhal' - Created task run 'load-0' for task 'load'\n",
      "14:30:02.789 | INFO    | Task run 'load-0' - Running...\n",
      "Saving processed data to ../../data/churn_processed.parquet...\n",
      "Data saved successfully.\n",
      "14:30:03.123 | INFO    | Task run 'load-0' - Finished in state Completed()\n",
      "14:30:03.456 | INFO    | Flow run 'maroon-narwhal' - Finished in state Completed('All states completed.')\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    etl_flow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Conclusion\n",
    "\n",
    "Congratulations! You have successfully built and run a scalable ETL pipeline using Prefect and Dask. You've seen how to break a pipeline into tasks and orchestrate them with a flow. You've also seen how Dask can be used to parallelize computations.\n",
    "\n",
    "In the next lab, we will use the `churn_processed.parquet` file we just created to run an AutoML experiment and find the best model for predicting customer churn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
